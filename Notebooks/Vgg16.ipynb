{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7892e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.applications import vgg16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7213fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/shirinshujaa/Furniture_Data\"\n",
    "desired_size = (256, 256)\n",
    "\n",
    "image_data = []\n",
    "image_hashes = set()\n",
    "image_color_hist = []\n",
    "\n",
    "main_folder_name = os.path.basename(data_dir)\n",
    "# Define the selected categories\n",
    "selected_categories = ['sofas', 'beds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "223a75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(dataset_folder, desired_size, max_images_per_category):\n",
    "    image_data = []\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_folder):\n",
    "        for parent_folder in dirs:\n",
    "            parent_folder_path = os.path.join(root, parent_folder)\n",
    "            \n",
    "            category = os.path.basename(os.path.dirname(parent_folder_path))\n",
    "            style = os.path.basename(parent_folder_path)\n",
    "\n",
    "            category_images = []\n",
    "\n",
    "            for filename in os.listdir(parent_folder_path):\n",
    "                if filename == \".DS_Store\":\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(parent_folder_path, filename)\n",
    "\n",
    "                if os.path.isdir(file_path):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    img = cv2.imread(file_path)\n",
    "                    if img is None:\n",
    "                        raise Exception(\"Image is corrupted or invalid\")\n",
    "                    resized_img = cv2.resize(img, desired_size)\n",
    "                    category_images.append((resized_img, category, style))  # Include category and style\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {file_path}: {e}\")\n",
    "\n",
    "            # Sample a subset of images per category\n",
    "            sampled_images = random.sample(category_images, min(max_images_per_category, len(category_images)))\n",
    "            image_data.extend(sampled_images)\n",
    "\n",
    "            # Display some sample images\n",
    "            # if num_sample_images > 0 and sampled_images:\n",
    "            #     print(f\"Sample images from category: {category}, style: {style}\")\n",
    "            #     fig, axes = plt.subplots(1, min(len(sampled_images), num_sample_images), figsize=(12, 4))\n",
    "            #     for i in range(min(len(sampled_images), num_sample_images)):\n",
    "            #         title = f\"Category: {category.capitalize()}, Style: {style.capitalize()}\"\n",
    "            #         axes[i].imshow(cv2.cvtColor(sampled_images[i][0], cv2.COLOR_BGR2RGB))\n",
    "            #         axes[i].set_title(title)\n",
    "            #         axes[i].axis('off')\n",
    "            #     plt.show()\n",
    "\n",
    "    return image_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e8d0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess images\n",
    "data_dir = \"/Users/shirinshujaa/Furniture_Data\"\n",
    "desired_size = (256, 256)\n",
    "max_images_per_category = 100  # Define the maximum number of images per category\n",
    "num_sample_images=3\n",
    "image_data = preprocess_images(data_dir, desired_size, max_images_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52ab747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "X = np.array([entry[0] for entry in image_data])\n",
    "y = np.array([entry[1] for entry in image_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99ff0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b422181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33a212dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VGG16 base model without the top (fully connected) layers\n",
    "def create_vgg16_base(input_shape):\n",
    "    vgg_base = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in vgg_base.layers:\n",
    "        layer.trainable = False  # Freeze VGG16 base layers\n",
    "    return vgg_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c99870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VGG16 base\n",
    "input_shape = X_train.shape[1:]  # Input shape excluding batch dimension\n",
    "vgg_base = create_vgg16_base(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24fac087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom head for the model\n",
    "def create_custom_head(bottom_model, num_classes):\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(1024, activation='relu')(top_model)\n",
    "    top_model = Dense(512, activation='relu')(top_model)\n",
    "    top_model = Dense(num_classes, activation='softmax')(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dcab91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom head\n",
    "num_classes = y_categorical.shape[1]  # Number of classes in your dataset\n",
    "custom_head = create_custom_head(vgg_base, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f4e5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine VGG16 base and custom head to create the final model\n",
    "model = Model(inputs=vgg_base.input, outputs=custom_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fd46405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c930fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 3s/step - accuracy: 0.7629 - loss: 1.6386 - val_accuracy: 0.9006 - val_loss: 0.2968\n",
      "Epoch 2/5\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m711s\u001b[0m 3s/step - accuracy: 0.9391 - loss: 0.1778 - val_accuracy: 0.8994 - val_loss: 0.3291\n",
      "Epoch 3/5\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 3s/step - accuracy: 0.9511 - loss: 0.1318 - val_accuracy: 0.8836 - val_loss: 0.3969\n",
      "Epoch 4/5\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 4s/step - accuracy: 0.9623 - loss: 0.1059 - val_accuracy: 0.9188 - val_loss: 0.3042\n",
      "Epoch 5/5\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m751s\u001b[0m 4s/step - accuracy: 0.9738 - loss: 0.0771 - val_accuracy: 0.9127 - val_loss: 0.4049\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2c811a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 3s/step - accuracy: 0.9034 - loss: 0.4054\n",
      "Test Loss: 0.40491169691085815\n",
      "Test Accuracy: 0.9127272963523865\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3420c118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"furniture_classification_model_Vgg16.h5\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
