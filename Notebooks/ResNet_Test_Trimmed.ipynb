{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7750b6d",
   "metadata": {},
   "source": [
    "Assume you are a team of machine learning engineers working for an ecommerce furniture shop, where users can browse and navigate interior furniture items. You are required to build a Furniture Recommender that allows users who have recently moved to explore furniture on your ecommerce system at ease. Your systems should have a functionality to help users navigate to the category of the furniture item that users want to buy. In most of the current online shops, users should type the name of the items and browse from the list of the results. However, to enhance the quality of the searching results, our system provides an image based searching function, where the users can upload the images of the furniture item that they are looking for. The system will accomplish an image search and return the list of similar-styled furniture in favor from our dataset.\n",
    "In the Furniture dataset, there are 06 categories: beds - 6578 images; chairs - 22053 images; dressers - 7871 images; lamps - 32402 images; sofas - 4080 images; tables - 17100 images, with total of 90084 images. For every category, there are 17 interior styles:\n",
    "- (a) Asian; (b) Beach; (c) Contemp; (d) Craftsman; (e) Eclectic; (f) Farmhouse; \n",
    "- (g) Industrial; (h) Media; (i) Midcentury; (j) Modern; (k) Rustic; (l) Scandinavian; \n",
    "- (m) Southwestern; (n) Traditional; (o) Transitional; (p) Tropical and (q) Victorian\n",
    "\n",
    "You have three tasks in this project:\n",
    "- **Task 1:** Classify images according to furniture category (beds; chairs; dressers; lamps; sofas; tables)\n",
    "- **Task 2:** Recommend 10 furniture items in our dataset which is similar to the input furniture item image from users. You are required to define a metric of “similarity” between two furniture items.\n",
    "- **Task 3:** (only for those aim HD) The extension of the model in Task 2, the recommended furniture items must be in the same interior styles with the style of the input images. In order to fulfill this task, you are required to build a model to recognize the style of a furniture item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86885d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641dac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"../Data/Raw/Furniture_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3375b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_size = (256, 256)\n",
    "\n",
    "image_data = []\n",
    "image_hashes = set()\n",
    "image_color_hist = []\n",
    "\n",
    "main_folder_name = os.path.basename(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa070e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(dataset_folder):\n",
    "    for parent_folder in dirs:\n",
    "        parent_folder_path = os.path.join(root, parent_folder)\n",
    "        \n",
    "        for filename in os.listdir(parent_folder_path):\n",
    "            if filename == \".DS_Store\":\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(parent_folder_path, filename)\n",
    "            \n",
    "            if os.path.isdir(file_path):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "\n",
    "                img = Image.open(file_path)\n",
    "                resized_img = img.resize(desired_size)\n",
    "                image_hash = hashlib.md5(resized_img.tobytes()).hexdigest()\n",
    "                parent_folder_dir = os.path.dirname(parent_folder_path)\n",
    "                parent_folder_name = os.path.basename(parent_folder_dir)\n",
    "                \n",
    "                # Ignore duplicates\n",
    "                if image_hash not in image_hashes:\n",
    "                    # Add new img to hash\n",
    "                    image_hashes.add(image_hash)\n",
    "                    subfolder_name = os.path.basename(parent_folder_path)\n",
    "                    image_data.append((parent_folder_name, subfolder_name, resized_img))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0583c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85165\n",
      "85165\n"
     ]
    }
   ],
   "source": [
    "print(len(image_data))\n",
    "print(len(image_hashes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "707e006c-7ea5-4caf-adfa-d8ac8892c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using these 2 only because I picked chairs back when we were doing the EDA and sofas is the smallest category\n",
    "only_these_cats = [\"chairs\", \"sofas\"]\n",
    "mini_dataset = [(category, style, img) for category, style, img in image_data if category in only_these_cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f96c848-2a6a-4c0f-8c61-76f4755b336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25924\n"
     ]
    }
   ],
   "source": [
    "print(len(mini_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39dd5909-4e5e-4068-8c22-6e7f5bf690e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesses data: resize image, normalize\n",
    "# Also implements batch sizes since the dataset's gonna be humongous.\n",
    "# Instead of cramming all data from the dataset to train the model, each time, it'll take a portion of the data\n",
    "# as big as the specified batch_size and use it for training.\n",
    "class CustomDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        \n",
    "        # Had to encode label because it was still \"chairs\" and \"sofas\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = [category for category, _, _ in self.data]\n",
    "        self.labels_encoded = self.label_encoder.fit_transform(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_indexes]\n",
    "        images, labels = [], []\n",
    "\n",
    "        # Ignoring the furniture's style for now\n",
    "        for category, _, image in batch_data:\n",
    "            # Resized to 224 x 224 because ResNet likes it that way\n",
    "            image = image.resize((224, 224))\n",
    "            image = np.array(image) / 255.0\n",
    "            images.append(image)\n",
    "        labels = self.labels_encoded[batch_indexes]\n",
    "        return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d5b40d8-06dd-4c51-85b1-47add89bc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (60), validation (20), and testing (20)\n",
    "train_data, remaining_data = train_test_split(mini_dataset, test_size=0.4, shuffle=True)\n",
    "val_data, test_data = train_test_split(remaining_data, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba65c155-f172-41b9-b1cb-632c39887425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Data generators based on original dataset and batch size\n",
    "train_generator = CustomDataset(train_data, batch_size)\n",
    "val_generator = CustomDataset(val_data, batch_size)\n",
    "test_generator = CustomDataset(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efd24836-1595-419e-b9ac-fba797346c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# Might have to be tuned later on\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16960d51-7590-489b-aaf9-8ca32cee48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "layer_flat_dense = layers.Flatten()(model.output)\n",
    "layer_flat_dense = layers.Dense(256, activation='relu')(layer_flat_dense)\n",
    "output = layers.Dense(len(only_these_cats), activation='softmax')(layer_flat_dense)\n",
    "\n",
    "model = models.Model(inputs=model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6f67f74-35de-45c8-9a3f-e12f16ea6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e49dd38-7af0-4d87-9e7d-a42ae643a3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "486/486 [==============================] - 734s 2s/step - loss: 0.4449 - accuracy: 0.9039 - val_loss: 0.1848 - val_accuracy: 0.9356\n",
      "Epoch 2/10\n",
      "486/486 [==============================] - 746s 2s/step - loss: 0.1653 - accuracy: 0.9461 - val_loss: 0.1534 - val_accuracy: 0.9506\n",
      "Epoch 3/10\n",
      "486/486 [==============================] - 747s 2s/step - loss: 0.1502 - accuracy: 0.9464 - val_loss: 0.1384 - val_accuracy: 0.9549\n",
      "Epoch 4/10\n",
      "486/486 [==============================] - 747s 2s/step - loss: 0.1211 - accuracy: 0.9593 - val_loss: 0.1238 - val_accuracy: 0.9599\n",
      "Epoch 5/10\n",
      "486/486 [==============================] - 750s 2s/step - loss: 0.1088 - accuracy: 0.9633 - val_loss: 0.2564 - val_accuracy: 0.8818\n",
      "Epoch 6/10\n",
      "486/486 [==============================] - 772s 2s/step - loss: 0.1045 - accuracy: 0.9644 - val_loss: 0.1194 - val_accuracy: 0.9610\n",
      "Epoch 7/10\n",
      "486/486 [==============================] - 763s 2s/step - loss: 0.0915 - accuracy: 0.9678 - val_loss: 0.1375 - val_accuracy: 0.9551\n",
      "Epoch 8/10\n",
      "486/486 [==============================] - 749s 2s/step - loss: 0.0864 - accuracy: 0.9691 - val_loss: 0.1164 - val_accuracy: 0.9655\n",
      "Epoch 9/10\n",
      "486/486 [==============================] - 745s 2s/step - loss: 0.0831 - accuracy: 0.9706 - val_loss: 0.1046 - val_accuracy: 0.9689\n",
      "Epoch 10/10\n",
      "486/486 [==============================] - 746s 2s/step - loss: 0.0798 - accuracy: 0.9719 - val_loss: 0.1039 - val_accuracy: 0.9689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a43447eb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "results = model.fit(train_generator, epochs=num_epochs, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4806bfd-e7a8-4dcf-beab-95cc1d8608e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['accuracy'], label = 'train_acc')\n",
    "plt.plot(results.history['val_accuracy'], label = 'val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fef2fc-6604-4c6e-864b-3cc3099034b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['loss'], label = 'train_loss')\n",
    "plt.plot(results.history['val_loss'], label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
